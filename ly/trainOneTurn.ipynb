{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import lightgbm as lgb\n",
    "import gc\n",
    "import re\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from time import ctime\n",
    "import os, sys\n",
    "import json\n",
    "import jieba\n",
    "\n",
    "# 数据路径\n",
    "import path_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loads(item):\n",
    "    try:\n",
    "        return json.loads(item)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return json.loads(\"{}\")\n",
    "\n",
    "def min_edit(str1, str2):\n",
    "    str1 = str1.lower()\n",
    "    str2 = str2.lower()\n",
    "    \"\"\"计算两句子的最小编辑距离\"\"\"\n",
    "    matrix = [[i + j for j in range(len(str2) + 1)] for i in range(len(str1) + 1)]\n",
    "    for i in range(1, len(str1) + 1):\n",
    "        for j in range(1, len(str2) + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                d = 0\n",
    "            else:\n",
    "                d = 1\n",
    "            matrix[i][j] = min(matrix[i - 1][j] + 1, matrix[i][j - 1] + 1, matrix[i - 1][j - 1] + d)\n",
    "    return matrix[len(str1)][len(str2)]\n",
    "\n",
    "def get_prefix_loc_in_title(prefix,title):\n",
    "    \"\"\"计算查询词prefix出现在title中的那个位置，前、后、中、没出现\"\"\"\n",
    "    prefix = prefix.lower()\n",
    "    title = title.lower()\n",
    "    if prefix not in title:\n",
    "        return -1\n",
    "    lens = len(prefix)\n",
    "    if prefix == title[:lens]:\n",
    "        return 0\n",
    "    elif prefix == title[-lens:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def get_rp_prefix_in_title(prefix, title, mode='char'):\n",
    "    \"\"\"计算title对prefix的词、字级别的召回率、精确率\"\"\"\n",
    "    prefix = prefix.lower()\n",
    "    title = title.lower()\n",
    "    if mode == 'char':\n",
    "        prefix = list(prefix)\n",
    "        title = list(title)\n",
    "    else:\n",
    "        prefix = list(jieba.cut(prefix))\n",
    "        title = list(jieba.cut(title))\n",
    "    len_title = len(title)\n",
    "    len_prefix = len(prefix)\n",
    "    len_comm_xx = len(set(prefix) & set(title))\n",
    "\n",
    "    recall = len_comm_xx / (len_prefix + 0.01)\n",
    "    precision = len_comm_xx / (len_title + 0.01)\n",
    "    acc = len_comm_xx / (len_title + len_prefix - len_comm_xx)\n",
    "    return [recall, precision, acc]\n",
    "\n",
    "def get_ngram_rp_prefix_in_title(prefix, title, mode='char'):\n",
    "    \"\"\"计算title对prefix的词、字级别的召回率、精确率（1-2gram）\"\"\"\n",
    "    prefix = prefix.lower()\n",
    "    title = title.lower()\n",
    "    if mode == 'char':\n",
    "        prefix = list(prefix)\n",
    "        title = list(title)\n",
    "    else:\n",
    "        prefix = list(jieba.cut(prefix))\n",
    "        title = list(jieba.cut(title))\n",
    "    prefix_2gram = []\n",
    "    for i in range(len(prefix) - 1):\n",
    "        prefix_2gram.append(prefix[i] + prefix[i + 1])\n",
    "    prefix.extend(prefix_2gram)\n",
    "\n",
    "    title_2gram = []\n",
    "    for i in range(len(title) - 1):\n",
    "        title_2gram.append(title[i] + title[i + 1])\n",
    "    title.extend(title_2gram)\n",
    "\n",
    "    len_title = len(title)\n",
    "    len_prefix = len(prefix)\n",
    "    len_comm_xx = len(set(prefix) & set(title))\n",
    "\n",
    "    recall = len_comm_xx / (len_prefix + 0.01)\n",
    "    precision = len_comm_xx / (len_title + 0.01)\n",
    "    acc = len_comm_xx / (len_title + len_prefix - len_comm_xx)\n",
    "    return [recall, precision, acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt = path_file.train_txt\n",
    "test_txt = path_file.test_txt\n",
    "print(test_txt)\n",
    "val_txt = path_file.val_txt\n",
    "query_prediction_feature_txt = path_file.query_prediction_feature_txt\n",
    "title_prediction_jaccard_distance_txt = path_file.title_prediction_jaccard_distance_new_txt\n",
    "title_prediction_distance_txt = path_file.title_prediction_distance_new_txt\n",
    "title_tag_word2vec_distance_txt = path_file.title_tag_word2vec_distance_new_txt\n",
    "model_path = path_file.model_path\n",
    "# nlp_feat_txt = path_file.nlp_feat_txt\n",
    "nlp_feat_txt = path_file.nlp_feat_txt\n",
    "prefix_title_tfidf_txt = path_file.prefix_title_tfidf_txt\n",
    "\n",
    "test_query_prediction_feature_txt = '../DataSets/feat_data/test_B/test_query_prediction_feature4.txt'\n",
    "test_title_prediction_jaccard_distance_txt = '../DataSets/feat_data/test_B/test_title_prediction_jaccard_distance_new.txt'\n",
    "test_title_prediction_distance_txt = '../DataSets/feat_data/test_B/test_word_title_prediction_distance_new.txt'\n",
    "test_title_tag_word2vec_distance_txt = '../DataSets/feat_data/test_B/test_word_title_tag_word2vec_distance_new_new.txt'\n",
    "test_nlp_feat_txt = '../DataSets/feat_data/test_B/nlp_feat_test_B.txt'\n",
    "test_prefix_title_tfidf_txt = '../DataSets/feat_data/test_B/prefix_title_tfidf_feat_all_new.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table(train_txt,\n",
    "                           names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, encoding='utf-8',\n",
    "                           quoting=3).astype(str)\n",
    "val_data = pd.read_table(val_txt,\n",
    "                         names=['prefix', 'query_prediction', 'title', 'tag', 'label'], header=None, encoding='utf-8',\n",
    "                         quoting=3).astype(str)\n",
    "test_data = pd.read_table(test_txt,\n",
    "                          names=['prefix', 'query_prediction', 'title', 'tag'], header=None, encoding='utf-8',\n",
    "                          quoting=3).astype(str)\n",
    "# 去噪，只有一条数据\n",
    "train_data = train_data[train_data['label'].isin(['0', '1'])]\n",
    "\n",
    "# 统一赋值test_data的label\n",
    "test_data['label'] = -1\n",
    "\n",
    "len_train = train_data.shape[0]\n",
    "len_val = val_data.shape[0]\n",
    "len_test = test_data.shape[0]\n",
    "print(\"len_train\", len_train)\n",
    "print(\"len_val\", len_val)\n",
    "print(\"len_test\", len_test)\n",
    "\n",
    "# 连接在一起便于统一处理\n",
    "data = pd.concat([train_data, val_data, test_data], ignore_index=True)\n",
    "data['label'] = data['label'].apply(lambda x: int(x))\n",
    "#将没有query_prediction的条目填充为\"{}\"\n",
    "data['query_prediction'].replace('nan','{}',inplace=True)\n",
    "data['prefix'] = data['prefix'].apply(lambda x: x.lower())\n",
    "data['title'] = data['title'].apply(lambda x: x.lower())\n",
    "print('data shape : ', data.shape)\n",
    "\n",
    "train_data = data[:-(len_val + len_test)]\n",
    "print(train_data.shape)\n",
    "prefixs = train_data.groupby(['prefix'], as_index=False)['prefix'].agg({'cnt': 'count'})['prefix'].tolist()\n",
    "print('prefixs : ', len(prefixs))\n",
    "titles = train_data.groupby(['title'], as_index=False)['title'].agg({'cnt': 'count'})['title'].tolist()\n",
    "print('titles : ', len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['label'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[data['prefix'] == '抖'].shape)\n",
    "print(data[data['prefix'] == '淘'].shape)\n",
    "print(data[data['prefix'] == '酷狗'].shape)\n",
    "print(data[data['prefix'] == '西瓜'].shape)\n",
    "print(data[data['prefix'] == '万能'].shape)\n",
    "print(data[data['prefix'] == '瓜子二手车'].shape)\n",
    "\n",
    "data['prefix'] = data['prefix'].apply(lambda x: '抖音' if x == '抖' else x)\n",
    "data['prefix'] = data['prefix'].apply(lambda x: '淘宝' if x == '淘' else x)\n",
    "data['prefix'] = data['prefix'].apply(lambda x: '酷狗音乐' if x == '酷狗' else x)\n",
    "data['prefix'] = data['prefix'].apply(lambda x: '西瓜小' if x == '西瓜' else x)\n",
    "data['prefix'] = data['prefix'].apply(lambda x: 'wifi万能钥匙' if x == '万能' else x)\n",
    "data['prefix'] = data['prefix'].apply(lambda x: '瓜子' if x == '瓜子二手车' else x)\n",
    "\n",
    "print(data[data['prefix'] == '抖'].shape)\n",
    "print(data[data['prefix'] == '淘'].shape)\n",
    "print(data[data['prefix'] == '酷狗'].shape)\n",
    "print(data[data['prefix'] == '西瓜'].shape)\n",
    "print(data[data['prefix'] == '万能'].shape)\n",
    "print(data[data['prefix'] == '瓜子二手车'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data['label'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = data[-(len_val + len_test):-len_test]\n",
    "test_data = data[-len_test:]\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "print(val_data[val_data['prefix'].isin(prefixs)].shape)\n",
    "print(test_data[test_data['prefix'].isin(prefixs)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# basic feat\n",
    "#####################################################\n",
    "\n",
    "###title feat##########\n",
    "print (\"gen_basic_feat\")\n",
    "data['title_len'] = data['title'].apply(lambda x: len(str(x)))\n",
    "data['title_digit_len'] = data['title'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "data['title_alpha_len'] = data['title'].apply(lambda x: len(re.findall('[a-zA-Z]', x)))\n",
    "data['title_zh_len'] = data['title'].apply(lambda x: len(re.findall(u'([\\u4e00-\\u9fff])', x)))\n",
    "\n",
    "data['title_digit_rate'] = data['title_digit_len'] * 1.0 / data['title_len'] * 1.0\n",
    "data['title_digit_sub'] = data['title_len'] - data['title_digit_len']\n",
    "data['title_alpha_rate'] = data['title_alpha_len'] * 1.0 / data['title_len'] * 1.0\n",
    "data['title_alpha_sub'] = data['title_len'] - data['title_alpha_len']\n",
    "\n",
    "# query_prediction\n",
    "data['prediction_is_null'] = data['query_prediction'].apply(lambda x: 1 if x == '{}' else 0)\n",
    "data[\"query_prediction\"] = data[\"query_prediction\"].apply(loads)\n",
    "data['prediction_num'] = data['query_prediction'].map(lambda x: len(x.keys()))\n",
    "\n",
    "####prefix feat#########\n",
    "data['prefix_len'] = data['prefix'].apply(lambda x: len(str(x)))\n",
    "data['prefix_digit_len'] = data['prefix'].apply(lambda x: sum(c.isdigit() for c in x))\n",
    "data['prefix_alpha_len'] = data['prefix'].apply(lambda x: len(re.findall('[a-zA-Z]', x)))\n",
    "data['prefix_zh_len'] = data['prefix'].apply(lambda x: len(re.findall(u'([\\u4e00-\\u9fff])', x)))\n",
    "\n",
    "data['prefix_digit_rate'] = data['prefix_digit_len'] * 1.0 / data['prefix_len'] * 1.0\n",
    "data['prefix_digit_sub'] = data['prefix_len'] - data['prefix_digit_len']\n",
    "data['prefix_alpha_rate'] = data['prefix_alpha_len'] * 1.0 / data['prefix_len'] * 1.0\n",
    "data['prefix_alpha_sub'] = data['prefix_len'] - data['prefix_alpha_len']\n",
    "\n",
    "######title_prefix feat##########\n",
    "# prefix_title_tfidf_feat = pd.read_csv(prefix_title_tfidf_txt, sep='\\t')[:-len_test]\n",
    "# # prefix_title_tfidf_feat = prefix_title_tfidf_feat\n",
    "# prefix_title_tfidf_feat_test = pd.read_csv(test_prefix_title_tfidf_txt, sep='\\t')\n",
    "# print('prefix_title_tfidf_feat_test:' , prefix_title_tfidf_feat_test.shape)\n",
    "# prefix_title_tfidf_feat = pd.concat([prefix_title_tfidf_feat,prefix_title_tfidf_feat_test], ignore_index=True)\n",
    "# print('prefix_title_tfidf_feat:' , prefix_title_tfidf_feat.shape)\n",
    "prefix_title_tfidf_feat = pd.read_csv(test_prefix_title_tfidf_txt, sep='\\t')\n",
    "print('prefix_title_tfidf_feat:' , prefix_title_tfidf_feat.shape)\n",
    "data = pd.concat([data, prefix_title_tfidf_feat], axis=1)\n",
    "del prefix_title_tfidf_feat\n",
    "# del prefix_title_tfidf_feat_test\n",
    "gc.collect()\n",
    "print('data shape : ', data.shape)\n",
    "\n",
    "data['prefix_is_title'] = data.apply(lambda x: 1 if x['prefix'].lower() == x['title'].lower() else 0, axis=1)\n",
    "# data['prefix_is_title'] = data.apply(lambda x: 1 if x['prefix'] == x['title'] else 0, axis=1)\n",
    "data['common_words_cnt'] = data.apply(\n",
    "    lambda x: len(set(list(str(x['prefix']).lower())).intersection(set(list(str(x['title']).lower())))), axis=1)\n",
    "# data['common_words_cnt'] = data.apply(\n",
    "#     lambda x: len(set(list(str(x['prefix']))).intersection(set(list(str(x['title']))))), axis=1)\n",
    "data['common_words_cnt_rate'] = data['common_words_cnt'] / data['title_len']\n",
    "data['prefix_len_rate'] = data['prefix_len'] / data['title_len']\n",
    "data['prefix_len_sub'] = data['prefix_len'] - data['title_len']\n",
    "data['prefix_is_titles_prefix'] = data.apply(lambda x: 1 if str(x['title']).lower().startswith(str(x['prefix']).lower()) else 0, axis=1)\n",
    "# data['prefix_is_titles_prefix'] = data.apply(lambda x: 1 if str(x['title']).startswith(str(x['prefix'])) else 0, axis=1)\n",
    "data['min_edit'] = data.apply(lambda x: min_edit(x['prefix'],x['title']), axis=1)\n",
    "data['prefix_loc'] = data.apply(lambda x: get_prefix_loc_in_title(x['prefix'],x['title']), axis=1)\n",
    "\n",
    "char_level_prefix = data.apply(lambda x: get_rp_prefix_in_title(x['prefix'],x['title'],mode='char'), axis=1)\n",
    "char_level_prefix = [kk for kk in char_level_prefix]\n",
    "char_level_prefix = np.array(char_level_prefix)\n",
    "data['prefix_t_recall_char'] = char_level_prefix[:,0].tolist()\n",
    "data['prefix_t_precision_char'] = char_level_prefix[:,1].tolist()\n",
    "data['prefix_t_acc_char'] = char_level_prefix[:,2].tolist()\n",
    "\n",
    "word_level_prefix = data.apply(lambda x: get_rp_prefix_in_title(x['prefix'], x['title'], mode='word'), axis=1)\n",
    "word_level_prefix = [kk for kk in word_level_prefix]\n",
    "word_level_prefix = np.array(word_level_prefix)\n",
    "data['prefix_t_recall_word'] = word_level_prefix[:, 0].tolist()\n",
    "data['prefix_t_precision_word'] = word_level_prefix[:, 1].tolist()\n",
    "data['prefix_t_acc_word'] = word_level_prefix[:, 2].tolist()\n",
    "\n",
    "char_ngram_level_prefix = data.apply(lambda x: get_ngram_rp_prefix_in_title(x['prefix'], x['title'], mode='char'), axis=1)\n",
    "char_ngram_level_prefix = [kk for kk in char_ngram_level_prefix]\n",
    "char_ngram_level_prefix = np.array(char_ngram_level_prefix)\n",
    "data['prefix_t_recall_char_ngram'] = char_ngram_level_prefix[:, 0].tolist()\n",
    "data['prefix_t_precision_char_ngram'] = char_ngram_level_prefix[:, 1].tolist()\n",
    "data['prefix_t_acc_char_ngram'] = char_ngram_level_prefix[:, 2].tolist()\n",
    "\n",
    "word_ngram_level_prefix = data.apply(lambda x: get_ngram_rp_prefix_in_title(x['prefix'], x['title'], mode='word'), axis=1)\n",
    "word_ngram_level_prefix = [kk for kk in word_ngram_level_prefix]\n",
    "word_ngram_level_prefix = np.array(word_ngram_level_prefix)\n",
    "data['prefix_t_recall_word_ngram'] = word_ngram_level_prefix[:, 0].tolist()\n",
    "data['prefix_t_precision_word_ngram'] = word_ngram_level_prefix[:, 1].tolist()\n",
    "data['prefix_t_acc_word_ngram'] = word_ngram_level_prefix[:, 2].tolist()\n",
    "\n",
    "######query predict feat#######\n",
    "\n",
    "##需要先跑其他两个.py文件（gen_query_prediction_feat.py 和 gen_query_prediction_feat2.py），才会生成以下两个.txt文件\n",
    "query_prediction_feature = pd.read_csv(query_prediction_feature_txt, sep='\\t')[:-len_test]\n",
    "# query_prediction_feature = query_prediction_feature[:-len_test]\n",
    "print('query_prediction_feature:' , query_prediction_feature.shape)\n",
    "query_prediction_feature_test = pd.read_csv(test_query_prediction_feature_txt, sep='\\t')\n",
    "print('query_prediction_feature_test:' , query_prediction_feature_test.shape)\n",
    "query_prediction_feature = pd.concat([query_prediction_feature,query_prediction_feature_test], ignore_index=True)\n",
    "print('query_prediction_feature:' , query_prediction_feature.shape)\n",
    "\n",
    "data = pd.concat([data, query_prediction_feature], axis=1)\n",
    "del query_prediction_feature\n",
    "del query_prediction_feature_test\n",
    "gc.collect()\n",
    "print('data shape : ', data.shape)\n",
    "#######word2vec distance#######\n",
    "####nlp 相关特征，暂时没有怎么做，以上都是统计特征\n",
    "####这部分特征，就是计算title prefix predict_query的word embedding的距离，\n",
    "####分词用的是thulac，word2vec用的是前几天腾讯开源的中文embedding，地址https://ai.tencent.com/ailab/nlp/embedding.html\n",
    "####在这个baseline的性能上没有提升多少，不到1个点，可以暂时忽略，跑之前也需要生成以下三个txt文件##\n",
    "nlp_feat = pd.read_csv(nlp_feat_txt, sep='\\t')[:-len_test]\n",
    "# nlp_feat = nlp_feat[:-len_test]\n",
    "print('nlp_feat:' , nlp_feat.shape)\n",
    "nlp_feat_test = pd.read_csv(test_nlp_feat_txt, sep='\\t')\n",
    "print('nlp_feat_test:' , nlp_feat_test.shape)\n",
    "nlp_feat = pd.concat([nlp_feat,nlp_feat_test], ignore_index=True)\n",
    "print('nlp_feat:' , nlp_feat.shape)\n",
    "\n",
    "data = pd.concat([data, nlp_feat], axis=1)\n",
    "del nlp_feat\n",
    "del nlp_feat_test\n",
    "gc.collect()\n",
    "print('data shape : ', data.shape)\n",
    "\n",
    "title_prediction_jaccard_distance = pd.read_csv(title_prediction_jaccard_distance_txt, sep='\\t')[:-len_test]\n",
    "# title_prediction_jaccard_distance = title_prediction_jaccard_distance[:-len_test]\n",
    "print('title_prediction_jaccard_distance:' , title_prediction_jaccard_distance.shape)\n",
    "title_prediction_jaccard_distance_test = pd.read_csv(test_title_prediction_jaccard_distance_txt, sep='\\t')\n",
    "print('title_prediction_jaccard_distance_test:' , title_prediction_jaccard_distance_test.shape)\n",
    "title_prediction_jaccard_distance = pd.concat([title_prediction_jaccard_distance,title_prediction_jaccard_distance_test], ignore_index=True)\n",
    "print('title_prediction_jaccard_distance:' , title_prediction_jaccard_distance.shape)\n",
    "\n",
    "data = pd.concat([data, title_prediction_jaccard_distance], axis=1)\n",
    "del title_prediction_jaccard_distance\n",
    "del title_prediction_jaccard_distance_test\n",
    "gc.collect()\n",
    "print('data shape : ', data.shape)\n",
    "\n",
    "title_prediction_distance = pd.read_csv(title_prediction_distance_txt, sep='\\t')[:-len_test]\n",
    "# title_prediction_distance = title_prediction_distance[:-len_test]\n",
    "print('title_prediction_distance:' , title_prediction_distance.shape)\n",
    "title_prediction_distance_test = pd.read_csv(test_title_prediction_distance_txt, sep='\\t')\n",
    "print('title_prediction_distance_test:' , title_prediction_distance_test.shape)\n",
    "title_prediction_distance = pd.concat([title_prediction_distance,title_prediction_distance_test], ignore_index=True)\n",
    "print('title_prediction_distance:' , title_prediction_distance.shape)\n",
    "\n",
    "data = pd.concat([data, title_prediction_distance], axis=1)\n",
    "del title_prediction_distance\n",
    "del title_prediction_distance_test\n",
    "gc.collect()\n",
    "print('data shape : ', data.shape)\n",
    "\n",
    "title_tag_word2vec_distance = pd.read_csv(title_tag_word2vec_distance_txt, sep='\\t')[:-len_test]\n",
    "# title_tag_word2vec_distance = title_tag_word2vec_distance[:-len_test]\n",
    "print('title_tag_word2vec_distance:' , title_tag_word2vec_distance.shape)\n",
    "title_tag_word2vec_distance_test = pd.read_csv(test_title_tag_word2vec_distance_txt, sep='\\t')\n",
    "print('title_tag_word2vec_distance_test:' , title_tag_word2vec_distance_test.shape)\n",
    "title_tag_word2vec_distance = pd.concat([title_tag_word2vec_distance,title_tag_word2vec_distance_test], ignore_index=True)\n",
    "print('title_tag_word2vec_distance:' , title_tag_word2vec_distance.shape)\n",
    "\n",
    "data = pd.concat([data, title_tag_word2vec_distance], axis=1)\n",
    "del title_tag_word2vec_distance\n",
    "del title_tag_word2vec_distance_test\n",
    "gc.collect()\n",
    "print('data shape : ', data.shape)\n",
    "\n",
    "# print (\"title_prediction_distance bigram\")\n",
    "# title_prediction_distance = pd.read_csv(title_prediction_bigram_distance_txt, sep='\\t')\n",
    "# data = pd.concat([data, title_prediction_distance[:-len_test]], axis=1)\n",
    "# del title_prediction_distance\n",
    "# gc.collect()\n",
    "\n",
    "# print (\"title_tag_word2vec_distance bigram\")\n",
    "# title_tag_word2vec_distance = pd.read_csv(title_tag_word2vec_distance_bigram_txt, sep='\\t')\n",
    "# data = pd.concat([data, title_tag_word2vec_distance[:-len_test]], axis=1)\n",
    "# del title_tag_word2vec_distance\n",
    "# gc.collect()\n",
    "\n",
    "#######CTR feat##########\n",
    "\n",
    "#####感觉 CTR特征很容易过拟合，尤其是数据量少的时候，所以再跑的时候没有用######\n",
    "\n",
    "# 打乱数据并分成num_folds份\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "del train_data\n",
    "del val_data\n",
    "del test_data\n",
    "gc.collect()\n",
    "\n",
    "train_data = data[:-(len_val + len_test)]\n",
    "val_data_ctr = data[-(len_val + len_test):]\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "train_data = shuffle(train_data, random_state=103)\n",
    "\n",
    "num_folds = 5\n",
    "step = int(len_train / num_folds) + 1\n",
    "data_slices = []\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "items = ['prefix', 'title', 'tag']\n",
    "len_items = ['prefix_len', 'prediction_num']\n",
    "# def get_ctr_feat(feat_data,tmp_data):\n",
    "#     for item in items:\n",
    "#         temp = feat_data.groupby(item, as_index=False)['label'].agg({item + '_click': 'sum', item + '_count': 'count'})\n",
    "#         temp[item + '_ctr'] = temp[item + '_click'] / (temp[item + '_count'])\n",
    "#         tmp_data = pd.merge(tmp_data, temp, on=item, how='left')\n",
    "#         tmp_data = tmp_data.fillna(0)\n",
    "#         mm = MinMaxScaler()\n",
    "#         tmp_data[[item + '_click',item + '_count']] = mm.fit_transform(tmp_data[[item + '_click',item + '_count']])\n",
    "#\n",
    "#     print('2 cross')\n",
    "#     for i in range(len(items)):\n",
    "#         for j in range(i + 1, len(items)):\n",
    "#             item_g = [items[i], items[j]]\n",
    "#             temp = feat_data.groupby(item_g, as_index=False)['label'].agg(\n",
    "#                 {'_'.join(item_g) + '_click': 'sum', '_'.join(item_g) + '_count': 'count'})\n",
    "#             temp['_'.join(item_g) + '_ctr'] = temp['_'.join(item_g) + '_click'] / (\n",
    "#                 temp['_'.join(item_g) + '_count'] + 3)\n",
    "#             tmp_data = pd.merge(tmp_data, temp, on=item, how='left')\n",
    "#\n",
    "#             tmp_data = tmp_data.fillna(0)\n",
    "#             mm = MinMaxScaler()\n",
    "#             tmp_data[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']] = mm.fit_transform(\n",
    "#                 tmp_data[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']])\n",
    "#     return tmp_data\n",
    "#\n",
    "# folds = KFold(len_train_val,n_folds = num_folds,shuffle = True,random_state=42)\n",
    "# for curr_fold, (idx_train,idx_val) in enumerate(folds):\n",
    "#     print (\"curr fold : \",curr_fold)\n",
    "#     feat_data = train_val_data.loc[idx_train].copy()\n",
    "#     tmp_data = train_val_data.loc[idx_val].copy()\n",
    "#     data_slices.append(get_ctr_feat(feat_data,tmp_data))\n",
    "\n",
    "for i in range(0, len_train, step):\n",
    "    ith_data = train_data[i:i + step]\n",
    "    print (ith_data.shape[0])\n",
    "    rest_data = pd.concat([train_data[:i], train_data[i + step:]])\n",
    "\n",
    "    for item in len_items:\n",
    "        temp = rest_data.groupby(item, as_index=False)['label'].agg({item + '_click': 'sum', item + '_count': 'count'})\n",
    "        temp[item + '_ctr'] = temp[item + '_click'] / (temp[item + '_count'])\n",
    "        ith_data = pd.merge(ith_data, temp, on=item, how='left')\n",
    "        ith_data = ith_data.fillna(0)\n",
    "        mm = MinMaxScaler()\n",
    "        ith_data[[item + '_click', item + '_count']] = mm.fit_transform(ith_data[[item + '_click', item + '_count']])\n",
    "\n",
    "        item_g = [item, 'tag']\n",
    "        temp = rest_data.groupby(item_g, as_index=False)['label'].agg(\n",
    "            {'_'.join(item_g) + '_click': 'sum', '_'.join(item_g) + '_count': 'count'})\n",
    "        temp['_'.join(item_g) + '_ctr'] = temp['_'.join(item_g) + '_click'] / (\n",
    "            temp['_'.join(item_g) + '_count'] + 3)\n",
    "        ith_data = pd.merge(ith_data, temp, on=item_g, how='left')\n",
    "        ith_data = ith_data.fillna(0)\n",
    "        mm = MinMaxScaler()\n",
    "        ith_data[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']] = mm.fit_transform(\n",
    "            ith_data[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']])\n",
    "\n",
    "    for item in items:\n",
    "        temp = rest_data.groupby(item, as_index=False)['label'].agg({item + '_click': 'sum', item + '_count': 'count'})\n",
    "        temp[item + '_ctr'] = temp[item + '_click'] / (temp[item + '_count'])\n",
    "        ith_data = pd.merge(ith_data, temp, on=item, how='left')\n",
    "        ith_data = ith_data.fillna(0)\n",
    "        mm = MinMaxScaler()\n",
    "        ith_data[[item + '_click', item + '_count']] = mm.fit_transform(ith_data[[item + '_click', item + '_count']])\n",
    "    print('2 cross')\n",
    "    for i in range(len(items)):\n",
    "        for j in range(i + 1, len(items)):\n",
    "            item_g = [items[i], items[j]]\n",
    "            temp = rest_data.groupby(item_g, as_index=False)['label'].agg(\n",
    "                {'_'.join(item_g) + '_click': 'sum', '_'.join(item_g) + '_count': 'count'})\n",
    "            temp['_'.join(item_g) + '_ctr'] = temp['_'.join(item_g) + '_click'] / (\n",
    "                temp['_'.join(item_g) + '_count'] + 3)\n",
    "            ith_data = pd.merge(ith_data, temp, on=item_g, how='left')\n",
    "            ith_data = ith_data.fillna(0)\n",
    "            mm = MinMaxScaler()\n",
    "            ith_data[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']] = mm.fit_transform(\n",
    "                ith_data[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']])\n",
    "    print('3 cross')\n",
    "    item_g = ['prefix', 'title', 'tag']\n",
    "    temp = rest_data.groupby(item_g, as_index=False)['label'].agg(\n",
    "        {'_'.join(item_g) + '_click': 'sum', '_'.join(item_g) + '_count': 'count'})\n",
    "    temp['_'.join(item_g) + '_ctr'] = temp['_'.join(item_g) + '_click'] / (\n",
    "        temp['_'.join(item_g) + '_count'] + 3)\n",
    "    ith_data = pd.merge(ith_data, temp, on=item_g, how='left')\n",
    "    ith_data = ith_data.fillna(0)\n",
    "    mm = MinMaxScaler()\n",
    "    ith_data[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']] = mm.fit_transform(\n",
    "        ith_data[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']])\n",
    "    data_slices.append(ith_data)\n",
    "\n",
    "train_data_ctr = pd.concat(data_slices, ignore_index=True)\n",
    "del data_slices\n",
    "gc.collect()\n",
    "del ith_data\n",
    "del rest_data\n",
    "gc.collect()\n",
    "\n",
    "for item in len_items:\n",
    "    print(item)\n",
    "    temp = train_data.groupby(item, as_index=False)['label'].agg({item + '_click': 'sum', item + '_count': 'count'})\n",
    "    temp[item + '_ctr'] = temp[item + '_click'] / (temp[item + '_count'])\n",
    "    temp[item + '_click'] = temp[item + '_click'] / num_folds * (num_folds - 1)\n",
    "    temp[item + '_count'] = temp[item + '_count'] / num_folds * (num_folds - 1)\n",
    "    val_data_ctr = pd.merge(val_data_ctr, temp, on=item, how='left')\n",
    "    val_data_ctr = val_data_ctr.fillna(0)\n",
    "    mm = MinMaxScaler()\n",
    "    val_data_ctr[[item + '_click', item + '_count']] = mm.fit_transform(\n",
    "        val_data_ctr[[item + '_click', item + '_count']])\n",
    "    item_g = [item, 'tag']\n",
    "    temp = train_data.groupby(item_g, as_index=False)['label'].agg(\n",
    "        {'_'.join(item_g) + '_click': 'sum', '_'.join(item_g) + '_count': 'count'})\n",
    "    temp['_'.join(item_g) + '_ctr'] = temp['_'.join(item_g) + '_click'] / (temp['_'.join(item_g) + '_count'] + 3)\n",
    "    temp['_'.join(item_g) + '_click'] = temp['_'.join(item_g) + '_click'] / num_folds * (num_folds - 1)\n",
    "    temp['_'.join(item_g) + '_count'] = temp['_'.join(item_g) + '_count'] / num_folds * (num_folds - 1)\n",
    "    val_data_ctr = pd.merge(val_data_ctr, temp, on=item_g, how='left')\n",
    "\n",
    "    val_data_ctr = val_data_ctr.fillna(0)\n",
    "    mm = MinMaxScaler()\n",
    "    val_data_ctr[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']] = mm.fit_transform(\n",
    "        val_data_ctr[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']])\n",
    "\n",
    "for item in items:\n",
    "    print(item)\n",
    "    temp = train_data.groupby(item, as_index=False)['label'].agg({item + '_click': 'sum', item + '_count': 'count'})\n",
    "    temp[item + '_ctr'] = temp[item + '_click'] / (temp[item + '_count'])\n",
    "    temp[item + '_click'] = temp[item + '_click'] / num_folds * (num_folds - 1)\n",
    "    temp[item + '_count'] = temp[item + '_count'] / num_folds * (num_folds - 1)\n",
    "    val_data_ctr = pd.merge(val_data_ctr, temp, on=item, how='left')\n",
    "    val_data_ctr = val_data_ctr.fillna(0)\n",
    "    mm = MinMaxScaler()\n",
    "    val_data_ctr[[item + '_click', item + '_count']] = mm.fit_transform(\n",
    "        val_data_ctr[[item + '_click', item + '_count']])\n",
    "print('2 cross')\n",
    "for i in range(len(items)):\n",
    "    for j in range(i + 1, len(items)):\n",
    "        print(items[i], ' ', items[j])\n",
    "        item_g = [items[i], items[j]]\n",
    "        temp = train_data.groupby(item_g, as_index=False)['label'].agg(\n",
    "            {'_'.join(item_g) + '_click': 'sum', '_'.join(item_g) + '_count': 'count'})\n",
    "        temp['_'.join(item_g) + '_ctr'] = temp['_'.join(item_g) + '_click'] / (temp['_'.join(item_g) + '_count'] + 3)\n",
    "        temp['_'.join(item_g) + '_click'] = temp['_'.join(item_g) + '_click'] / num_folds * (num_folds - 1)\n",
    "        temp['_'.join(item_g) + '_count'] = temp['_'.join(item_g) + '_count'] / num_folds * (num_folds - 1)\n",
    "        val_data_ctr = pd.merge(val_data_ctr, temp, on=item_g, how='left')\n",
    "\n",
    "        val_data_ctr = val_data_ctr.fillna(0)\n",
    "        mm = MinMaxScaler()\n",
    "        val_data_ctr[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']] = mm.fit_transform(\n",
    "            val_data_ctr[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']])\n",
    "\n",
    "print('3 cross')\n",
    "item_g = ['prefix', 'title', 'tag']\n",
    "temp = train_data.groupby(item_g, as_index=False)['label'].agg(\n",
    "    {'_'.join(item_g) + '_click': 'sum', '_'.join(item_g) + '_count': 'count'})\n",
    "temp['_'.join(item_g) + '_ctr'] = temp['_'.join(item_g) + '_click'] / (temp['_'.join(item_g) + '_count'] + 3)\n",
    "temp['_'.join(item_g) + '_click'] = temp['_'.join(item_g) + '_click'] / num_folds * (num_folds - 1)\n",
    "temp['_'.join(item_g) + '_count'] = temp['_'.join(item_g) + '_count'] / num_folds * (num_folds - 1)\n",
    "val_data_ctr = pd.merge(val_data_ctr, temp, on=item_g, how='left')\n",
    "val_data_ctr = val_data_ctr.fillna(0)\n",
    "mm = MinMaxScaler()\n",
    "val_data_ctr[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']] = mm.fit_transform(\n",
    "    val_data_ctr[['_'.join(item_g) + '_click', '_'.join(item_g) + '_count']])\n",
    "\n",
    "data = pd.concat([train_data_ctr, val_data_ctr])\n",
    "del train_data\n",
    "del train_data_ctr\n",
    "del val_data_ctr\n",
    "del temp\n",
    "gc.collect()\n",
    "\n",
    "def encode_count(df, column_name, new_column_name=''):\n",
    "    lbl = preprocessing.LabelEncoder()\n",
    "    lbl.fit(list(df[column_name].values))\n",
    "    if new_column_name == '':\n",
    "        df[column_name] = lbl.transform(list(df[column_name].values))\n",
    "    else:\n",
    "        df[new_column_name] = lbl.transform(list(df[column_name].values))\n",
    "    return df\n",
    "\n",
    "\n",
    "encoder_col = ['prefix', 'title', 'tag']\n",
    "for col in encoder_col:\n",
    "    print(col)\n",
    "    data = encode_count(data, col, col + '_id')\n",
    "\n",
    "drop_feat = [\n",
    "    'query_prediction_js',\n",
    "    'prefix',\n",
    "    'query_prediction',\n",
    "    'title',\n",
    "    'tag',\n",
    "    'label',\n",
    "    'prediction_num', # query_prediction_feature里面已经存在了\n",
    "    'prefix_len_click',\n",
    "    # 'prefix_len_count',\n",
    "    # 'prefix_len_ctr',\n",
    "    'prediction_num_click',\n",
    "    # 'prediction_num_count',\n",
    "    # 'prediction_num_ctr',\n",
    "    'prefix_len_tag_click',\n",
    "    # 'prefix_len_tag_count',\n",
    "    # 'prefix_len_tag_ctr',\n",
    "    'prediction_num_tag_click',\n",
    "    # 'prediction_num_tag_count',\n",
    "    # 'prediction_num_tag_ctr',\n",
    "    'prefix_click',\n",
    "    #  'prefix_count',\n",
    "    #  'prefix_ctr',\n",
    "    'title_click',\n",
    "    #  'title_count',\n",
    "    #  'title_ctr',\n",
    "    'tag_click',\n",
    "    #  'tag_count',\n",
    "    #  'tag_ctr',\n",
    "    'query_prediction_click',\n",
    "#     'query_prediction_count',\n",
    "#     'query_prediction_ctr',\n",
    "    'prefix_title_click',\n",
    "    #  'prefix_title_count',\n",
    "    #  'prefix_title_ctr',\n",
    "    'prefix_tag_click',\n",
    "    #  'prefix_tag_count',\n",
    "    #  'prefix_tag_ctr',\n",
    "    'prefix_query_prediction_click',\n",
    "    'prefix_query_prediction_count',\n",
    "    'prefix_query_prediction_ctr',\n",
    "    'title_tag_click',\n",
    "    #  'title_tag_count',\n",
    "    #  'title_tag_ctr',\n",
    "    'title_query_prediction_click',\n",
    "#      'title_query_prediction_count',\n",
    "#     'title_query_prediction_ctr',\n",
    "    'tag_query_prediction_click',\n",
    "#      'tag_query_prediction_count',\n",
    "#     'tag_query_prediction_ctr',\n",
    "    'prefix_title_tag_click',\n",
    "    #  'prefix_title_tag_count',\n",
    "    #  'prefix_title_tag_ctr',\n",
    "    # 'prefix_len_mul_pre_std',\n",
    "    'title_prefix_jaccard_distance',\n",
    "    'title_premean_jaccard_distance',\n",
    "    'title_premean_jaccard_distance',\n",
    "    'title_prewmean_jaccard_distance',\n",
    "    'title_prewmean_jaccard_distance',\n",
    "    'title_premean_jaccard_distance',\n",
    "    'title_premean_jaccard_distance',\n",
    "    'title_prewmean_jaccard_distance',\n",
    "    'title_prewmean_jaccard_distance'\n",
    "]\n",
    "\n",
    "drop_feat2 = [\n",
    "    'query_prediction_js',\n",
    "    'query_prediction_id',\n",
    "    'prefix_id',\n",
    "    'in_query_big',\n",
    "    'query_t_recall_char',\n",
    "    'query_t_precision_char',\n",
    "    'query_t_acc_char',\n",
    "    'query_t_recall_word',\n",
    "    'query_t_precision_word',\n",
    "    'query_t_acc_word',\n",
    "    'query_t_recall_char_ngram',\n",
    "    'query_t_precision_char_ngram',\n",
    "    'query_t_acc_char_ngram',\n",
    "    'query_t_recall_word_ngram',\n",
    "    'query_t_precision_word_ngram',\n",
    "    'query_t_acc_word_ngram',\n",
    "    'prefix',\n",
    "    'query_prediction',\n",
    "    'title',\n",
    "    'tag',\n",
    "    'label',\n",
    "    'prediction_num',\n",
    "    'prefix_len_click',\n",
    "    # 'prefix_len_count',\n",
    "    # 'prefix_len_ctr',\n",
    "    'prediction_num_click',\n",
    "    # 'prediction_num_count',\n",
    "    # 'prediction_num_ctr',\n",
    "    'prefix_len_tag_click',\n",
    "    # 'prefix_len_tag_count',\n",
    "    # 'prefix_len_tag_ctr',\n",
    "    'prediction_num_tag_click',\n",
    "    # 'prediction_num_tag_count',\n",
    "    # 'prediction_num_tag_ctr',\n",
    "    'prefix_click',\n",
    "    'prefix_count',\n",
    "    'prefix_ctr',\n",
    "    'title_click',\n",
    "#     'title_count',\n",
    "#     'title_ctr',\n",
    "    'tag_click',\n",
    "    #  'tag_count',\n",
    "    #  'tag_ctr',\n",
    "    'query_prediction_click',\n",
    "    'query_prediction_count',\n",
    "    'query_prediction_ctr',\n",
    "    'prefix_title_click',\n",
    "    'prefix_title_count',\n",
    "    'prefix_title_ctr',\n",
    "    'prefix_tag_click',\n",
    "    'prefix_tag_count',\n",
    "    'prefix_tag_ctr',\n",
    "    'prefix_query_prediction_click',\n",
    "    'prefix_query_prediction_count',\n",
    "    'prefix_query_prediction_ctr',\n",
    "    'title_tag_click',\n",
    "    'title_tag_count',\n",
    "    'title_tag_ctr',\n",
    "    'title_query_prediction_click',\n",
    "    'title_query_prediction_count',\n",
    "    'title_query_prediction_ctr',\n",
    "    'tag_query_prediction_click',\n",
    "    'tag_query_prediction_count',\n",
    "    'tag_query_prediction_ctr',\n",
    "    'prefix_title_tag_click',\n",
    "    'prefix_title_tag_count',\n",
    "    'prefix_title_tag_ctr',\n",
    "    # 'prefix_len_mul_pre_std',s\n",
    "    'title_prefix_jaccard_distance',\n",
    "    'title_premean_jaccard_distance',\n",
    "    'title_premean_jaccard_distance',\n",
    "    'title_prewmean_jaccard_distance',\n",
    "    'title_prewmean_jaccard_distance',\n",
    "    'title_premean_jaccard_distance',\n",
    "    'title_premean_jaccard_distance',\n",
    "    'title_prewmean_jaccard_distance',\n",
    "    'title_prewmean_jaccard_distance'\n",
    "]\n",
    "\n",
    "categorical_columns_can = []\n",
    "\n",
    "drop_cols_can = drop_feat\n",
    "drop_cols = []\n",
    "data_feat = list(data.columns)\n",
    "categorical_columns = []\n",
    "for i in categorical_columns_can:\n",
    "    if i in data_feat and i not in drop_cols:\n",
    "        categorical_columns.append(i)\n",
    "\n",
    "for i in drop_cols_can:\n",
    "    if i in data_feat:\n",
    "        drop_cols.append(i)\n",
    "\n",
    "drop_cols_can2 = drop_feat2\n",
    "drop_cols2 = []\n",
    "\n",
    "for i in drop_cols_can2:\n",
    "    if i in data_feat:\n",
    "        drop_cols2.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_round = 100000\n",
    "# 阈值\n",
    "online_threshold = 0.38\n",
    "\n",
    "train_feat = data[:-(len_val + len_test)].copy()\n",
    "val_feat = data[-(len_val + len_test):-len_test].copy()\n",
    "test_feat = data[-len_test:].copy()\n",
    "\n",
    "print(data['label'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_feat.shape)\n",
    "print(val_feat.shape)\n",
    "print('test shape:', test_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_colums = list(train_feat.drop(drop_cols, axis=1).columns)\n",
    "print(len(feat_colums))\n",
    "# print(feat_colums)\n",
    "\n",
    "feat_colums2 = list(train_feat.drop(drop_cols2, axis=1).columns)\n",
    "print(len(feat_colums2))\n",
    "# print(feat_colums2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train model 1\n",
    "data_x = train_feat.drop(drop_cols, axis=1)\n",
    "data_y = train_feat['label'].values\n",
    "\n",
    "# train model 2\n",
    "data_x2 = train_feat.drop(drop_cols2, axis=1)\n",
    "data_y2 = data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_feat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = data_x.values\n",
    "data_x2 = data_x2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_feat_in = val_feat[val_feat['prefix'].isin(prefixs)]\n",
    "real_val_x_in = val_feat_in.drop(drop_cols, axis=1).values\n",
    "real_val_y_in = val_feat_in['label'].values\n",
    "real_val_x_in_2 = val_feat_in.drop(drop_cols2, axis=1).values\n",
    "\n",
    "val_feat_not_in = val_feat[~val_feat['prefix'].isin(prefixs)]\n",
    "real_val_x_not_in = val_feat_not_in.drop(drop_cols2, axis=1).values\n",
    "real_val_y_not_in = val_feat_not_in['label'].values\n",
    "real_val_x_not_in_2 = val_feat_not_in.drop(drop_cols, axis=1).values\n",
    "\n",
    "real_val_x_1 = np.concatenate([real_val_x_in, real_val_x_not_in_2])\n",
    "real_val_x = np.concatenate([real_val_x_in_2, real_val_x_not_in])\n",
    "real_val_y = np.concatenate([real_val_y_in, real_val_y_not_in])\n",
    "\n",
    "test_x_in = test_feat.drop(drop_cols, axis=1).values\n",
    "test_x_not_in = test_feat.drop(drop_cols2, axis=1).values\n",
    "print (\"test is in prefix : \", test_feat[test_feat['prefix'].isin(prefixs)].shape)\n",
    "\n",
    "val_not_in_prefix_in_title = val_feat_not_in[val_feat_not_in['title'].isin(titles)].drop(drop_cols2, axis=1)\n",
    "val_not_in_prefix_in_title_label = val_feat_not_in[val_feat_not_in['title'].isin(titles)]['label'].values\n",
    "print(val_not_in_prefix_in_title.shape)\n",
    "val_not_in_prefix_not_in_title = val_feat_not_in[~val_feat_not_in['title'].isin(titles)].drop(drop_cols2, axis=1)\n",
    "val_not_in_prefix_not_in_title_label = val_feat_not_in[~val_feat_not_in['title'].isin(titles)]['label'].values\n",
    "print(val_not_in_prefix_not_in_title.shape)\n",
    "val_not_in_prefix_in_title = val_not_in_prefix_in_title.values\n",
    "val_not_in_prefix_not_in_title = val_not_in_prefix_not_in_title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_feat_in.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del val_feat\n",
    "del val_feat_in\n",
    "del val_feat_not_in\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "SEED = 100\n",
    "params = {\n",
    "                # num_leaves：128，256\n",
    "                'num_leaves': 2 ** 7 - 1,\n",
    "                'objective': 'binary',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'max_depth': -1,\n",
    "                'min_data_in_leaf': 50,\n",
    "                'learning_rate': 0.01,\n",
    "                'feature_fraction': 0.65,\n",
    "                'bagging_fraction': 0.75,\n",
    "                'bagging_freq': 1,\n",
    "                'metric': {'auc'},\n",
    "                'seed': SEED,\n",
    "                # 'scale_pos_weight':0.899844466771833,\n",
    "                # 'min_child_weight':5,\n",
    "                # 'min_split_gain':0,\n",
    "                # 'subsample_for_bin':50000,\n",
    "                'nthread': 15,\n",
    "                #                 'lambda_l1':3,\n",
    "                #                 'lambda_l2':2,\n",
    "                'max_bin': 1023,\n",
    "                # 'device': 'gpu'\n",
    "            }\n",
    "\n",
    "train_matrix = lgb.Dataset(pd.DataFrame(data_x, columns=feat_colums), label=data_y,\n",
    "                                       categorical_feature=categorical_columns)\n",
    "valid_matrix = lgb.Dataset(pd.DataFrame(real_val_x_in, columns=feat_colums), label=real_val_y_in,\n",
    "                                       categorical_feature=categorical_columns)\n",
    "\n",
    "early_stopping_rounds = 100\n",
    "model_auc = lgb.train(params, train_matrix, num_round, valid_sets=[valid_matrix],\n",
    "                              early_stopping_rounds=early_stopping_rounds,\n",
    "                              verbose_eval=500\n",
    "                              )\n",
    "best_iter_auc = model_auc.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_matrix\n",
    "del valid_matrix\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "                # num_leaves：128，256\n",
    "                'num_leaves': 2 ** 7 - 1,\n",
    "                'objective': 'binary',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'max_depth': -1,\n",
    "                'min_data_in_leaf': 50,\n",
    "                'learning_rate': lr,\n",
    "                'feature_fraction': 0.65,\n",
    "                'bagging_fraction': 0.75,\n",
    "                'bagging_freq': 1,\n",
    "                'metric': {'auc'},\n",
    "                'seed': SEED,\n",
    "                # 'scale_pos_weight':0.899844466771833,\n",
    "                # 'min_child_weight':5,\n",
    "                # 'min_split_gain':0,\n",
    "                # 'subsample_for_bin':50000,\n",
    "                'nthread': 15,\n",
    "                #                 'lambda_l1':3,\n",
    "                #                 'lambda_l2':2,\n",
    "                'max_bin': 1023,\n",
    "                # 'device': 'gpu'\n",
    "            }\n",
    "\n",
    "train_matrix2 = lgb.Dataset(pd.DataFrame(data_x2, columns=feat_colums2), label=data_y2,\n",
    "                                        categorical_feature=categorical_columns)\n",
    "valid_matrix2 = lgb.Dataset(pd.DataFrame(real_val_x, columns=feat_colums2), label=real_val_y,\n",
    "                                        categorical_feature=categorical_columns)\n",
    "model2_auc = lgb.train(params, train_matrix2, num_round, valid_sets=[valid_matrix2],\n",
    "                               early_stopping_rounds=early_stopping_rounds,\n",
    "                               verbose_eval=500\n",
    "                               )\n",
    "best_iter2_auc = model2_auc.best_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_matrix2\n",
    "del valid_matrix2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_iter2_auc)\n",
    "\n",
    "real_val_pred_in = model_auc.predict(real_val_x_in, num_iteration=best_iter_auc)\n",
    "real_val_pred_1 = model_auc.predict(real_val_x_1, num_iteration=best_iter_auc)\n",
    "print(classification_report(real_val_y, np.where(real_val_pred_1 > online_threshold, 1, 0), digits=6))\n",
    "print(classification_report(real_val_y_in, np.where(real_val_pred_in > online_threshold, 1, 0), digits=6))\n",
    "\n",
    "print(\"different threshold for real val pred in\")\n",
    "max_f1 = 0\n",
    "max_threshold_in = 0\n",
    "for threshold in np.arange(0.30, 0.60, 0.01):\n",
    "    tmp_f1 = f1_score(real_val_y_in, np.where(real_val_pred_in > threshold, 1, 0))\n",
    "    print(threshold, ' f1 score: ', tmp_f1)\n",
    "    if tmp_f1 > max_f1:\n",
    "        max_f1 = tmp_f1\n",
    "        max_threshold_in = threshold\n",
    "print('best threshold: ', max_threshold_in, '  f1 :', max_f1)\n",
    "\n",
    "real_val_pred_not_in = model2_auc.predict(real_val_x_not_in, num_iteration=best_iter2_auc)\n",
    "real_val_pred_2 = model2_auc.predict(real_val_x, num_iteration=best_iter2_auc)\n",
    "print(\n",
    "            classification_report(real_val_y_not_in, np.where(real_val_pred_not_in > online_threshold, 1, 0), digits=6))\n",
    "print(\n",
    "            classification_report(real_val_y, np.where(real_val_pred_2 > online_threshold, 1, 0), digits=6))\n",
    "print(\"different threshold for real val pred not in\")\n",
    "max_f1 = 0\n",
    "max_threshold_not_in = 0\n",
    "for threshold in np.arange(0.30, 0.60, 0.01):\n",
    "    tmp_f1 = f1_score(real_val_y_not_in, np.where(real_val_pred_not_in > threshold, 1, 0))\n",
    "    print(threshold, ' f1 score: ', tmp_f1)\n",
    "    if tmp_f1 > max_f1:\n",
    "        max_f1 = tmp_f1\n",
    "        max_threshold_not_in = threshold\n",
    "print('best threshold: ', max_threshold_not_in, '  f1 :', max_f1)\n",
    "\n",
    "# 测试不同的阈值组合\n",
    "print(\"different threshold for real val pred in and not\")\n",
    "max_f1 = 0\n",
    "max_threshold_in = 0\n",
    "max_threshold_not_in = 0\n",
    "for threshold_in in np.arange(0.30, 0.60, 0.01):\n",
    "    for threshold_not_in in np.arange(0.30, 0.60, 0.01):\n",
    "        pred_in = np.where(real_val_pred_in > threshold_in, 1, 0)\n",
    "        pred_not_in = np.where(real_val_pred_not_in > threshold_not_in, 1, 0)\n",
    "        pred = np.concatenate([pred_in, pred_not_in])\n",
    "        tmp_f1 = f1_score(real_val_y, pred)\n",
    "        #                     print(threshold_in,' ',threshold_not_in , ' f1 score: ', tmp_f1)\n",
    "        if tmp_f1 > max_f1:\n",
    "            print(threshold_in, ' ', threshold_not_in, ' f1 score: ', tmp_f1)\n",
    "            max_f1 = tmp_f1\n",
    "            max_threshold_in = threshold_in\n",
    "            max_threshold_not_in = threshold_not_in\n",
    "print('best threshold for real val: ', max_threshold_in, ' ', max_threshold_not_in, '  f1 :', max_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_val_pred_in = model_auc.predict(real_val_x_in, num_iteration=model_auc.best_iteration)\n",
    "real_val_pred_not_in = model2_auc.predict(real_val_x_not_in, num_iteration=model2_auc.best_iteration)\n",
    "\n",
    "real_val_pred = np.concatenate([real_val_pred_in, real_val_pred_not_in])\n",
    "pred_in = np.where(real_val_pred_in > 0.41, 1, 0)\n",
    "pred_not_in = np.where(real_val_pred_not_in > 0.37, 1, 0)\n",
    "pred = np.concatenate([pred_in, pred_not_in])\n",
    "\n",
    "print(f1_score(real_val_y,pred))\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['predicted_score'] = real_val_pred\n",
    "submission['label'] = pred\n",
    "\n",
    "submission.to_csv(\n",
    "            'pred_val_online_threshold_' + str(online_threshold) + '_feat_' + str(lr) + '_' + str(\n",
    "                len(feat_colums)) + '_' + str(\n",
    "                submission['predicted_score'].mean()) + '_' + str(submission['label'].sum()) + '.csv', sep=',', index=False)\n",
    "submission['label'].to_csv(\n",
    "            'result_val_online_threshold_' + str(online_threshold) + '_feat_' + str(lr) + '_' + str(\n",
    "                len(feat_colums)) + '_' + str(\n",
    "                submission['predicted_score'].mean()) + '_' + str(submission['label'].sum()) + '.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test_pred')\n",
    "# 利用模型对无标测试集做预测\n",
    "test_pred_in = model_auc.predict(test_x_in, num_iteration=model_auc.best_iteration)\n",
    "\n",
    "print('test_pred')\n",
    "# 利用模型对无标测试集做预测\n",
    "test_pred_not_in = model2_auc.predict(test_x_not_in, num_iteration=model2_auc.best_iteration)\n",
    "\n",
    "label_in = np.where(test_pred_in > 0.41, 1, 0)\n",
    "label_not_in = np.where(test_pred_not_in > 0.37, 1, 0)\n",
    "\n",
    "label = np.zeros((test_feat.shape[0],), dtype=int)\n",
    "\n",
    "label[test_feat['prefix'].isin(prefixs)] = label_in[test_feat['prefix'].isin(prefixs)]\n",
    "label[~test_feat['prefix'].isin(prefixs)] = label_not_in[~test_feat['prefix'].isin(prefixs)]\n",
    "\n",
    "predict = np.zeros((test_feat.shape[0],), dtype=float)\n",
    "\n",
    "predict[test_feat['prefix'].isin(prefixs)] = test_pred_in[test_feat['prefix'].isin(prefixs)]\n",
    "predict[~test_feat['prefix'].isin(prefixs)] = test_pred_not_in[~test_feat['prefix'].isin(prefixs)]\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['predicted_score'] = predict\n",
    "submission['label'] = label\n",
    "\n",
    "print(np.mean(submission['predicted_score']))\n",
    "print(np.sum(submission['label']))\n",
    "\n",
    "submission.to_csv(\n",
    "            'pred_online_threshold_' + str(online_threshold) + '_feat_' + str(lr) + '_' + str(\n",
    "                len(feat_colums)) + '_' + str(\n",
    "                submission['predicted_score'].mean()) + '_' + str(submission['label'].sum()) + '.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}